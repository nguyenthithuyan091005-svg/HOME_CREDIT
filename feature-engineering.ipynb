{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20ef1db8",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7761df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc8f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_final_selected_features.csv', 'application_test.csv', '.DS_Store', 'HomeCredit_columns_description.csv', 'POS_CASH_balance.csv', 'credit_card_balance.csv', 'installments_payments.csv', 'application_train.csv', 'bureau.csv', 'train_final_selected_features.csv', 'previous_application.csv', 'bureau_balance.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "IS_LOCAL = True\n",
    "\n",
    "import os\n",
    "\n",
    "if IS_LOCAL:\n",
    "    PATH = r\"/Users/trinhthilananh/Documents/Feature Engineering/home-credit-default-risk\"\n",
    "else:\n",
    "    PATH = \"../input\"\n",
    "\n",
    "print(os.listdir(PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ee3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = pd.read_csv(PATH+\"/application_train.csv\")\n",
    "application_test = pd.read_csv(PATH+\"/application_test.csv\")\n",
    "bureau = pd.read_csv(PATH+\"/bureau.csv\")\n",
    "bureau_balance = pd.read_csv(PATH+\"/bureau_balance.csv\")\n",
    "credit_card_balance = pd.read_csv(PATH+\"/credit_card_balance.csv\")\n",
    "installments_payments = pd.read_csv(PATH+\"/installments_payments.csv\")\n",
    "previous_application = pd.read_csv(PATH+\"/previous_application.csv\")\n",
    "POS_CASH_balance = pd.read_csv(PATH+\"/POS_CASH_balance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9c084c",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4cab1fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing Useful DataStructures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "#importing plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "#importing Misc Libraries\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "\n",
    "#for 100% jupyter notebook cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7cb5b2",
   "metadata": {},
   "source": [
    "## 1. X·ª≠ l√Ω b·∫£ng application_train.csv v√† application_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f9468c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K√≠ch th∆∞·ªõc df_app (g·ªôp): (356255, 122)\n",
      "K√≠ch th∆∞·ªõc df_app sau khi x·ª≠ l√Ω: (356255, 270)\n"
     ]
    }
   ],
   "source": [
    "# L∆∞u l·∫°i ID c·ªßa t·∫≠p test ƒë·ªÉ d√πng ·ªü cu·ªëi\n",
    "test_ids = application_test['SK_ID_CURR']\n",
    "\n",
    "# Th√™m c·ªôt TARGET v√†o t·∫≠p test (v·ªõi gi√° tr·ªã NaN) ƒë·ªÉ g·ªôp 2 file\n",
    "application_test['TARGET'] = np.nan\n",
    "\n",
    "# G·ªôp train v√† test\n",
    "df_app = pd.concat([application_train, application_test], ignore_index=True)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc df_app (g·ªôp): {df_app.shape}\")\n",
    "\n",
    "# --- Feature Engineering C∆° b·∫£n ---\n",
    "\n",
    "# 1. L√†m s·∫°ch d·ªØ li·ªáu b·∫•t th∆∞·ªùng (Anomaly)\n",
    "df_app['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
    "df_app['CODE_GENDER'].replace('XNA', np.nan, inplace=True)\n",
    "\n",
    "# 2. T·∫°o c√°c thu·ªôc t√≠nh t·ª∑ l·ªá (Domain Features)\n",
    "# Th√™m 0.00001 ƒë·ªÉ tr√°nh l·ªói chia cho 0\n",
    "df_app['CREDIT_INCOME_RATIO'] = df_app['AMT_CREDIT'] / (df_app['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "df_app['ANNUITY_INCOME_RATIO'] = df_app['AMT_ANNUITY'] / (df_app['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "df_app['CREDIT_ANNUITY_RATIO'] = df_app['AMT_CREDIT'] / (df_app['AMT_ANNUITY'] + 0.00001)\n",
    "df_app['CREDIT_GOODS_PRICE_RATIO'] = df_app['AMT_CREDIT'] / (df_app['AMT_GOODS_PRICE'] + 0.00001)\n",
    "df_app['DAYS_EMPLOYED_TO_BIRTH_RATIO'] = df_app['DAYS_EMPLOYED'] / (df_app['DAYS_BIRTH'] + 0.00001)\n",
    "\n",
    "# 3. K·∫øt h·ª£p c√°c thu·ªôc t√≠nh EXT_SOURCE\n",
    "df_app['EXT_SOURCES_MEAN'] = df_app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "df_app['EXT_SOURCES_PROD'] = df_app['EXT_SOURCE_1'] * df_app['EXT_SOURCE_2'] * df_app['EXT_SOURCE_3']\n",
    "df_app['EXT_SOURCES_MIN'] = df_app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].min(axis=1)\n",
    "df_app['EXT_SOURCES_MAX'] = df_app[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].max(axis=1)\n",
    "\n",
    "# 4. X·ª≠ l√Ω c√°c c·ªôt Categorical (d√πng One-Hot Encoding)\n",
    "# L·∫•y danh s√°ch c√°c c·ªôt 'object'\n",
    "categorical_cols = df_app.select_dtypes(include=['object']).columns\n",
    "\n",
    "# D√πng pd.get_dummies\n",
    "df_app = pd.get_dummies(df_app, columns=categorical_cols, dummy_na=True)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc df_app sau khi x·ª≠ l√Ω: {df_app.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3b511",
   "metadata": {},
   "source": [
    "## 2. X·ª≠ l√Ω bureau.csv v√† bureau_balance.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4941df20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1: X·ª≠ l√Ω bureau_balance\n",
      "ƒêang ch·∫°y pd.get_dummies cho c·ªôt 'STATUS'...\n",
      "C√°c c·ªôt STATUS s·∫Ω ƒë∆∞·ª£c t·ªïng h·ª£p: ['STATUS_nan', 'STATUS_C', 'STATUS_2', 'STATUS_0', 'STATUS_1', 'STATUS_X', 'STATUS_3', 'STATUS_5', 'STATUS_4']\n",
      "K√≠ch th∆∞·ªõc agg_bureau_balance (ƒë√£ agg): (817395, 46)\n",
      "5 c·ªôt ƒë·∫ßu ti√™n: ['SK_ID_BUREAU', 'STATUS_NAN_MEAN', 'STATUS_NAN_MIN', 'STATUS_NAN_MAX', 'STATUS_NAN_SUM']\n"
     ]
    }
   ],
   "source": [
    "print(\"2.1: X·ª≠ l√Ω bureau_balance\")\n",
    "\n",
    "# 1. X·ª≠ l√Ω c·ªôt STATUS (Ch·∫°y get_dummies m·ªôt c√°ch r√µ r√†ng)\n",
    "# Ch√∫ng ta s·∫Ω l∆∞u l·∫°i t√™n c√°c c·ªôt m·ªõi ƒë∆∞·ª£c t·∫°o\n",
    "original_cols = set(bureau_balance.columns)\n",
    "\n",
    "if 'STATUS' in bureau_balance.columns:\n",
    "    print(\"ƒêang ch·∫°y pd.get_dummies cho c·ªôt 'STATUS'...\")\n",
    "    # Th√™m dtype=int ƒë·ªÉ ƒë·∫£m b·∫£o c√°c c·ªôt m·ªõi l√† S·ªê\n",
    "    bureau_balance = pd.get_dummies(bureau_balance, columns=['STATUS'], dummy_na=True, dtype=int)\n",
    "    # L·∫•y danh s√°ch c√°c c·ªôt m·ªõi\n",
    "    new_status_cols = list(set(bureau_balance.columns) - original_cols)\n",
    "else:\n",
    "    print(\"C·ªôt 'STATUS' d∆∞·ªùng nh∆∞ ƒë√£ ƒë∆∞·ª£c get_dummies. ƒêang t√¨m c√°c c·ªôt STATUS_...\")\n",
    "    new_status_cols = [col for col in bureau_balance.columns if col.startswith('STATUS_')]\n",
    "\n",
    "# Ki·ªÉm tra xem c√≥ c·ªôt n√†o ƒë·ªÉ t·ªïng h·ª£p kh√¥ng\n",
    "if not new_status_cols:\n",
    "    print(\"!!! L·ªñI: Kh√¥ng t√¨m th·∫•y c·ªôt STATUS n√†o (STATUS_0, STATUS_C...) ƒë·ªÉ t·ªïng h·ª£p.\")\n",
    "    raise ValueError(\"Kh√¥ng t√¨m th·∫•y c·ªôt STATUS_... ƒë·ªÉ t·ªïng h·ª£p. Vui l√≤ng ki·ªÉm tra l·∫°i file bureau_balance.\")\n",
    "\n",
    "print(f\"C√°c c·ªôt STATUS s·∫Ω ƒë∆∞·ª£c t·ªïng h·ª£p: {new_status_cols}\")\n",
    "\n",
    "# 2. ƒê·ªãnh nghƒ©a c√°c ph√©p t·ªïng h·ª£p\n",
    "# Ch·ªâ t·ªïng h·ª£p tr√™n c√°c c·ªôt STATUS m·ªõi\n",
    "balance_aggregations = {}\n",
    "for col in new_status_cols:\n",
    "    balance_aggregations[col] = ['mean', 'min', 'max', 'sum', 'count']\n",
    "\n",
    "# 3. T·ªïng h·ª£p (Aggregate) theo SK_ID_BUREAU\n",
    "# D√≤ng n√†y s·∫Ω ho·∫°t ƒë·ªông v√¨ balance_aggregations KH√îNG r·ªóng\n",
    "agg_bureau_balance = bureau_balance.groupby('SK_ID_BUREAU').agg(balance_aggregations)\n",
    "\n",
    "# 4. GI·∫¢I PH√ÅP S·ª¨A L·ªñI (MultiIndex) - Gi·ªØ nguy√™n nh∆∞ tr∆∞·ªõc\n",
    "# Chuy·ªÉn 2 c·∫•p ƒë·ªô c·ªôt (v√≠ d·ª•: ('STATUS_0', 'MEAN')) v·ªÅ 1 c·∫•p ƒë·ªô ('STATUS_0_MEAN')\n",
    "agg_bureau_balance.columns = ['_'.join(col).upper() for col in agg_bureau_balance.columns.values]\n",
    "agg_bureau_balance.reset_index(inplace=True) # ƒê∆∞a SK_ID_BUREAU v·ªÅ l√†m c·ªôt\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc agg_bureau_balance (ƒë√£ agg): {agg_bureau_balance.shape}\")\n",
    "print(f\"5 c·ªôt ƒë·∫ßu ti√™n: {list(agg_bureau_balance.columns[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f108e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2: X·ª≠ l√Ω bureau...\n",
      "K√≠ch th∆∞·ªõc agg_bureau (ƒë√£ agg): (305811, 164)\n",
      "5 c·ªôt ƒë·∫ßu ti√™n: ['SK_ID_CURR', 'BUREAU_DAYS_CREDIT_MIN', 'BUREAU_DAYS_CREDIT_MAX', 'BUREAU_DAYS_CREDIT_MEAN', 'BUREAU_DAYS_CREDIT_COUNT']\n"
     ]
    }
   ],
   "source": [
    "print(\"2.2: X·ª≠ l√Ω bureau...\")\n",
    "\n",
    "# 1. H·ª£p nh·∫•t (merge) bureau v·ªõi agg_bureau_balance ƒë√£ t·∫°o ·ªü b∆∞·ªõc 2.1\n",
    "df_bureau = bureau.merge(agg_bureau_balance, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "# 2. X·ª≠ l√Ω categorical trong bureau\n",
    "bureau_categorical_cols = df_bureau.select_dtypes(include=['object']).columns\n",
    "df_bureau = pd.get_dummies(df_bureau, columns=bureau_categorical_cols, dummy_na=True)\n",
    "\n",
    "# 3. ƒê·ªãnh nghƒ©a c√°c ph√©p t·ªïng h·ª£p\n",
    "# L·∫•y c√°c c·ªôt s·ªë (numeric)\n",
    "numeric_aggregations = {\n",
    "    'DAYS_CREDIT': ['min', 'max', 'mean', 'count', 'std'], \n",
    "    'CREDIT_DAY_OVERDUE': ['mean', 'max', 'std'], \n",
    "    'AMT_CREDIT_SUM': ['sum', 'mean', 'max', 'std'], \n",
    "    'AMT_CREDIT_SUM_DEBT': ['sum', 'mean', 'std'], \n",
    "    'AMT_CREDIT_SUM_OVERDUE': ['sum', 'mean', 'std'],\n",
    "    'CNT_CREDIT_PROLONG': ['sum', 'mean', 'std'], \n",
    "}\n",
    "# L·∫•y c√°c c·ªôt categorical ƒë√£ one-hot (t·ª´ bureau v√† bureau_balance)\n",
    "categorical_cols = [col for col in df_bureau.columns if col.startswith(('CREDIT_ACTIVE_', 'CREDIT_CURRENCY_', 'CREDIT_TYPE_', 'STATUS_'))]\n",
    "for col in categorical_cols:\n",
    "    numeric_aggregations[col] = ['mean', 'sum'] # L·∫•y t·ª∑ l·ªá v√† s·ªë l∆∞·ª£ng\n",
    "\n",
    "# 4. T·ªïng h·ª£p (Aggregate) theo SK_ID_CURR\n",
    "agg_bureau = df_bureau.groupby('SK_ID_CURR').agg(numeric_aggregations)\n",
    "\n",
    "# 5. √ÅP D·ª§NG GI·∫¢I PH√ÅP S·ª¨A L·ªñI (ƒê·ªïi t√™n c·ªôt MultiIndex)\n",
    "agg_bureau.columns = ['BUREAU_' + '_'.join(col).upper() for col in agg_bureau.columns.values]\n",
    "agg_bureau.reset_index(inplace=True) # SK_ID_CURR v·ªÅ l√†m c·ªôt\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc agg_bureau (ƒë√£ agg): {agg_bureau.shape}\")\n",
    "print(f\"5 c·ªôt ƒë·∫ßu ti√™n: {list(agg_bureau.columns[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8971e02d",
   "metadata": {},
   "source": [
    "## 3. X·ª≠ l√Ω Previous_applicaiton.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "248c5682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3: X·ª≠ l√Ω previous_application...\n",
      "K√≠ch th∆∞·ªõc agg_previous (ƒë√£ agg): (338857, 58)\n"
     ]
    }
   ],
   "source": [
    "print(\"3: X·ª≠ l√Ω previous_application...\")\n",
    "\n",
    "# 1. L√†m s·∫°ch gi√° tr·ªã 365243\n",
    "previous_application['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "previous_application['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "previous_application['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "previous_application['DAYS_LAST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "previous_application['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "# 2. T·∫°o thu·ªôc t√≠nh m·ªõi\n",
    "previous_application['AMT_CREDIT_GOODS_PRICE_DIFF'] = previous_application['AMT_CREDIT'] - previous_application['AMT_GOODS_PRICE']\n",
    "\n",
    "# 3. X·ª≠ l√Ω categorical\n",
    "prev_categorical_cols = previous_application.select_dtypes(include=['object']).columns\n",
    "previous_application = pd.get_dummies(previous_application, columns=prev_categorical_cols, dummy_na=True)\n",
    "\n",
    "# 4. ƒê·ªãnh nghƒ©a c√°c ph√©p t·ªïng h·ª£p\n",
    "prev_aggregations = {\n",
    "    'AMT_ANNUITY': ['min', 'max', 'mean', 'sum', 'std'],\n",
    "    'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "    'AMT_CREDIT': ['min', 'max', 'mean', 'sum', 'std'],\n",
    "    'AMT_CREDIT_GOODS_PRICE_DIFF': ['mean', 'max', 'sum', 'std'],\n",
    "    'AMT_DOWN_PAYMENT': ['mean', 'sum', 'std'],\n",
    "    'DAYS_DECISION': ['min', 'max', 'mean', 'std'],\n",
    "    'CNT_PAYMENT': ['mean', 'sum', 'std'],\n",
    "}\n",
    "\n",
    "# L·∫•y c√°c c·ªôt categorical (v√≠ d·ª•: T·ª∑ l·ªá c√°c h·ª£p ƒë·ªìng b·ªã t·ª´ ch·ªëi/ch·∫•p nh·∫≠n)\n",
    "cat_cols = [col for col in previous_application.columns if col.startswith(('NAME_CONTRACT_STATUS_', 'CODE_REJECT_REASON_'))]\n",
    "for col in cat_cols:\n",
    "    prev_aggregations[col] = ['mean', 'sum']\n",
    "    \n",
    "# 5. T·ªïng h·ª£p (Aggregate) theo SK_ID_CURR\n",
    "agg_previous = previous_application.groupby('SK_ID_CURR').agg(prev_aggregations)\n",
    "\n",
    "# 6. √ÅP D·ª§NG GI·∫¢I PH√ÅP S·ª¨A L·ªñI (ƒê·ªïi t√™n c·ªôt MultiIndex)\n",
    "agg_previous.columns = ['PREV_' + '_'.join(col).upper() for col in agg_previous.columns.values]\n",
    "agg_previous.reset_index(inplace=True) # SK_ID_CURR v·ªÅ l√†m c·ªôt\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc agg_previous (ƒë√£ agg): {agg_previous.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8e859",
   "metadata": {},
   "source": [
    "## 4. X·ª≠ l√Ω installments_payments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c85ab3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. X·ª≠ l√Ω installments_payments...\n",
      "K√≠ch th∆∞·ªõc agg_installments (ƒë√£ agg): (339587, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"4. X·ª≠ l√Ω installments_payments...\")\n",
    "\n",
    "# 1. T·∫°o thu·ªôc t√≠nh m·ªõi (S·ªë ng√†y tr·∫£ ch·∫≠m, S·ªë ti·ªÅn tr·∫£ thi·∫øu)\n",
    "installments_payments['DAYS_PAYMENT_DIFF'] = installments_payments['DAYS_INSTALMENT'] - installments_payments['DAYS_ENTRY_PAYMENT']\n",
    "installments_payments['AMT_PAYMENT_DIFF'] = installments_payments['AMT_INSTALMENT'] - installments_payments['AMT_PAYMENT']\n",
    "\n",
    "# 2. ƒê·ªãnh nghƒ©a t·ªïng h·ª£p\n",
    "install_aggregations = {\n",
    "    'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "    'DAYS_PAYMENT_DIFF': ['mean', 'max', 'sum', 'std'],\n",
    "    'AMT_PAYMENT_DIFF': ['mean', 'max', 'sum', 'std'],\n",
    "    'AMT_INSTALMENT': ['mean', 'sum', 'std'],\n",
    "    'AMT_PAYMENT': ['mean', 'sum', 'std'],\n",
    "    'SK_ID_PREV': ['nunique'] # ƒê·∫øm s·ªë kho·∫£n vay\n",
    "}\n",
    "\n",
    "# 3. T·ªïng h·ª£p\n",
    "agg_installments = installments_payments.groupby('SK_ID_CURR').agg(install_aggregations)\n",
    "\n",
    "# 4. √ÅP D·ª§NG GI·∫¢I PH√ÅP S·ª¨A L·ªñI\n",
    "agg_installments.columns = ['INSTALL_' + '_'.join(col).upper() for col in agg_installments.columns.values]\n",
    "agg_installments.reset_index(inplace=True)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc agg_installments (ƒë√£ agg): {agg_installments.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2a2d32",
   "metadata": {},
   "source": [
    "## 5. X·ª≠ l√Ω credit_card_balance.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e12b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5: X·ª≠ l√Ω credit_card_balance...\n",
      "K√≠ch th∆∞·ªõc agg_credit_card (ƒë√£ agg): (103558, 34)\n"
     ]
    }
   ],
   "source": [
    "print(\"5: X·ª≠ l√Ω credit_card_balance...\")\n",
    "\n",
    "# 1. T·∫°o thu·ªôc t√≠nh m·ªõi\n",
    "credit_card_balance['BALANCE_LIMIT_RATIO'] = credit_card_balance['AMT_BALANCE'] / (credit_card_balance['AMT_CREDIT_LIMIT_ACTUAL'] + 0.00001)\n",
    "\n",
    "# 2. ƒê·ªãnh nghƒ©a t·ªïng h·ª£p\n",
    "cc_aggregations = {\n",
    "    'AMT_BALANCE': ['mean', 'max', 'sum', 'std'],\n",
    "    'AMT_CREDIT_LIMIT_ACTUAL': ['mean', 'max', 'std'],\n",
    "    'AMT_DRAWINGS_ATM_CURRENT': ['mean', 'max', 'sum', 'std'],\n",
    "    'AMT_DRAWINGS_CURRENT': ['mean', 'max', 'sum', 'std'],\n",
    "    'CNT_DRAWINGS_ATM_CURRENT': ['mean', 'max', 'sum', 'std'],\n",
    "    'CNT_INSTALMENT_MATURE_CUM': ['max', 'mean', 'std'],\n",
    "    'SK_DPD': ['mean', 'max', 'sum', 'std'], # S·ªë ng√†y qu√° h·∫°n\n",
    "    'SK_DPD_DEF': ['mean', 'max', 'sum', 'std'], # S·ªë ng√†y qu√° h·∫°n (nghi√™m tr·ªçng)\n",
    "    'BALANCE_LIMIT_RATIO': ['mean', 'max', 'std']\n",
    "}\n",
    "\n",
    "# 3. T·ªïng h·ª£p\n",
    "agg_credit_card = credit_card_balance.groupby('SK_ID_CURR').agg(cc_aggregations)\n",
    "\n",
    "# 4. √ÅP D·ª§NG GI·∫¢I PH√ÅP S·ª¨A L·ªñI\n",
    "agg_credit_card.columns = ['CC_' + '_'.join(col).upper() for col in agg_credit_card.columns.values]\n",
    "agg_credit_card.reset_index(inplace=True)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc agg_credit_card (ƒë√£ agg): {agg_credit_card.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98dcbd",
   "metadata": {},
   "source": [
    "## 6. X·ª≠ l√Ω POSH_CASH_balance.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b06ebf2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: X·ª≠ l√Ω POS_CASH_balance (Phi√™n b·∫£n s·ª≠a l·ªói KeyError)\n",
      "ƒêang ch·∫°y pd.get_dummies cho c·ªôt 'NAME_CONTRACT_STATUS'...\n",
      "K√≠ch th∆∞·ªõc agg_pos_cash (ƒë√£ agg): (337252, 41)\n"
     ]
    }
   ],
   "source": [
    "print(\"6: X·ª≠ l√Ω POS_CASH_balance (Phi√™n b·∫£n s·ª≠a l·ªói KeyError)\")\n",
    "\n",
    "# 1. X·ª≠ l√Ω categorical (M·ªôt c√°ch an to√†n)\n",
    "# KI·ªÇM TRA tr∆∞·ªõc khi ch·∫°y get_dummies\n",
    "if 'NAME_CONTRACT_STATUS' in POS_CASH_balance.columns:\n",
    "    print(\"ƒêang ch·∫°y pd.get_dummies cho c·ªôt 'NAME_CONTRACT_STATUS'...\")\n",
    "    POS_CASH_balance = pd.get_dummies(POS_CASH_balance, columns=['NAME_CONTRACT_STATUS'], dummy_na=True)\n",
    "else:\n",
    "    print(\"C·ªôt 'NAME_CONTRACT_STATUS' d∆∞·ªùng nh∆∞ ƒë√£ ƒë∆∞·ª£c get_dummies. B·ªè qua.\")\n",
    "# 2. ƒê·ªãnh nghƒ©a t·ªïng h·ª£p\n",
    "pos_aggregations = {\n",
    "    'MONTHS_BALANCE': ['min', 'max', 'mean', 'std'], \n",
    "    'CNT_INSTALMENT': ['min', 'max', 'mean', 'std'], \n",
    "    'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean', 'std'], \n",
    "    'SK_DPD': ['mean', 'max', 'sum', 'std'], \n",
    "    'SK_DPD_DEF': ['mean', 'max', 'sum', 'std'], \n",
    "}\n",
    "\n",
    "# L·∫•y c√°c c·ªôt categorical (ƒë√£ ƒë∆∞·ª£c one-hot)\n",
    "# D√≤ng n√†y s·∫Ω t√¨m c√°c c·ªôt 'NAME_CONTRACT_STATUS_Active' v.v.\n",
    "cat_cols = [col for col in POS_CASH_balance.columns if col.startswith('NAME_CONTRACT_STATUS_')]\n",
    "for col in cat_cols:\n",
    "    pos_aggregations[col] = ['mean', 'sum']\n",
    "\n",
    "# 3. T·ªïng h·ª£p\n",
    "agg_pos_cash = POS_CASH_balance.groupby('SK_ID_CURR').agg(pos_aggregations)\n",
    "\n",
    "# 4. √ÅP D·ª§NG GI·∫¢I PH√ÅP S·ª¨A L·ªñI (MultiIndex)\n",
    "agg_pos_cash.columns = ['POS_' + '_'.join(col).upper() for col in agg_pos_cash.columns.values]\n",
    "agg_pos_cash.reset_index(inplace=True)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc agg_pos_cash (ƒë√£ agg): {agg_pos_cash.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b4585",
   "metadata": {},
   "source": [
    "## 7. Merge c√°c b·∫£ng d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "628ae367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7: H·ª£p nh·∫•t (Merge) t·∫•t c·∫£...\n",
      "K√≠ch th∆∞·ªõc sau khi merge bureau: (356255, 433)\n",
      "K√≠ch th∆∞·ªõc sau khi merge previous: (356255, 490)\n",
      "K√≠ch th∆∞·ªõc sau khi merge installments: (356255, 506)\n",
      "K√≠ch th∆∞·ªõc sau khi merge credit card: (356255, 539)\n",
      "K√≠ch th∆∞·ªõc sau khi merge POS cash: (356255, 579)\n",
      "--- H·ª¢P NH·∫§T TH√ÄNH C√îNG, df_final ƒê√É ƒê∆Ø·ª¢C T·∫†O ---\n"
     ]
    }
   ],
   "source": [
    "print(\"7: H·ª£p nh·∫•t (Merge) t·∫•t c·∫£...\")\n",
    "\n",
    "# 1. H·ª£p nh·∫•t df_app (t·ª´ B∆∞·ªõc 1) v·ªõi agg_bureau (t·ª´ B∆∞·ªõc 2.2)\n",
    "df_final = df_app.merge(agg_bureau, on='SK_ID_CURR', how='left')\n",
    "print(f\"K√≠ch th∆∞·ªõc sau khi merge bureau: {df_final.shape}\")\n",
    "\n",
    "# 2. H·ª£p nh·∫•t v·ªõi agg_previous (t·ª´ B∆∞·ªõc 2.3)\n",
    "df_final = df_final.merge(agg_previous, on='SK_ID_CURR', how='left')\n",
    "print(f\"K√≠ch th∆∞·ªõc sau khi merge previous: {df_final.shape}\")\n",
    "\n",
    "# 3. H·ª£p nh·∫•t v·ªõi agg_installments (t·ª´ B∆∞·ªõc 2.4)\n",
    "df_final = df_final.merge(agg_installments, on='SK_ID_CURR', how='left')\n",
    "print(f\"K√≠ch th∆∞·ªõc sau khi merge installments: {df_final.shape}\")\n",
    "\n",
    "# 4. H·ª£p nh·∫•t v·ªõi agg_credit_card (t·ª´ B∆∞·ªõc 2.5)\n",
    "df_final = df_final.merge(agg_credit_card, on='SK_ID_CURR', how='left')\n",
    "print(f\"K√≠ch th∆∞·ªõc sau khi merge credit card: {df_final.shape}\")\n",
    "\n",
    "# 5. H·ª£p nh·∫•t v·ªõi agg_pos_cash (t·ª´ B∆∞·ªõc 2.6)\n",
    "df_final = df_final.merge(agg_pos_cash, on='SK_ID_CURR', how='left')\n",
    "print(f\"K√≠ch th∆∞·ªõc sau khi merge POS cash: {df_final.shape}\")\n",
    "\n",
    "print(\"--- H·ª¢P NH·∫§T TH√ÄNH C√îNG, df_final ƒê√É ƒê∆Ø·ª¢C T·∫†O ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4e3309",
   "metadata": {},
   "source": [
    "## 8. T·∫°o Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eee1f528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫Øt ƒë·∫ßu B∆∞·ªõc 4: T·∫°o c√°c thu·ªôc t√≠nh t∆∞∆°ng t√°c (Interaction Features)...\n",
      "K√≠ch th∆∞·ªõc df_final sau khi th√™m interaction features: (356255, 586)\n"
     ]
    }
   ],
   "source": [
    "print(\"B·∫Øt ƒë·∫ßu B∆∞·ªõc 4: T·∫°o c√°c thu·ªôc t√≠nh t∆∞∆°ng t√°c (Interaction Features)...\")\n",
    "\n",
    "# T∆∞∆°ng t√°c Thu nh·∫≠p v√† Kho·∫£n vay hi·ªán t·∫°i\n",
    "df_final['APP_INCOME_CREDIT_RATIO'] = df_final['AMT_INCOME_TOTAL'] / (df_final['AMT_CREDIT'] + 0.00001)\n",
    "df_final['APP_INCOME_ANNUITY_RATIO'] = df_final['AMT_INCOME_TOTAL'] / (df_final['AMT_ANNUITY'] + 0.00001)\n",
    "df_final['APP_PAYMENT_RATE'] = df_final['AMT_ANNUITY'] / (df_final['AMT_CREDIT'] + 0.00001) # T·ª∑ l·ªá tr·∫£ g√≥p\n",
    "\n",
    "# T∆∞∆°ng t√°c Thu nh·∫≠p v√† L·ªãch s·ª≠ Bureau (kho·∫£n vay c≈© ·ªü n∆°i kh√°c)\n",
    "df_final['BUREAU_INCOME_CREDIT_RATIO'] = df_final['BUREAU_AMT_CREDIT_SUM_SUM'] / (df_final['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "df_final['BUREAU_INCOME_DEBT_RATIO'] = df_final['BUREAU_AMT_CREDIT_SUM_DEBT_SUM'] / (df_final['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "\n",
    "# T∆∞∆°ng t√°c Thu nh·∫≠p v√† L·ªãch s·ª≠ Previous App (kho·∫£n vay c≈© ·ªü Home Credit)\n",
    "df_final['PREV_INCOME_CREDIT_RATIO'] = df_final['PREV_AMT_CREDIT_SUM'] / (df_final['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "\n",
    "# T∆∞∆°ng t√°c Thu nh·∫≠p v√† L·ªãch s·ª≠ Tr·∫£ g√≥p\n",
    "df_final['INSTALL_INCOME_PAYMENT_RATIO'] = df_final['INSTALL_AMT_PAYMENT_SUM'] / (df_final['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "\n",
    "print(f\"K√≠ch th∆∞·ªõc df_final sau khi th√™m interaction features: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aebd09e",
   "metadata": {},
   "source": [
    "## 9. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d31533b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd3491c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªçc Feature\n",
      "ƒê√£ lo·∫°i b·ªè 6 c·ªôt do thi·∫øu d·ªØ li·ªáu > 80%.\n",
      "ƒê√£ lo·∫°i b·ªè 35 c·ªôt do ch·ªâ c√≥ 1 gi√° tr·ªã duy nh·∫•t.\n",
      "--- K√≠ch th∆∞·ªõc df_final sau L·ªçc 'Th√¥': (356255, 545) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"L·ªçc Feature\")\n",
    "\n",
    "# T√°ch train_df t·∫°m th·ªùi ƒê·ªÇ FIT b·ªô l·ªçc (tr√°nh data leakage)\n",
    "# df_final l√∫c n√†y ƒë√£ c√≥ 586 features (bao g·ªìm c·∫£ interaction)\n",
    "train_df_for_selection = df_final[df_final['TARGET'].notnull()].copy()\n",
    "\n",
    "# === 1. L·ªçc c√°c c·ªôt thi·∫øu qu√° nhi·ªÅu d·ªØ li·ªáu ===\n",
    "missing_threshold = 0.8 # Ng∆∞·ª°ng 80%\n",
    "missing_counts = train_df_for_selection.isnull().mean()\n",
    "cols_to_drop_missing = missing_counts[missing_counts > missing_threshold].index\n",
    "\n",
    "# √Åp d·ª•ng l·ªçc tr√™n df_final\n",
    "df_final = df_final.drop(columns=cols_to_drop_missing)\n",
    "print(f\"ƒê√£ lo·∫°i b·ªè {len(cols_to_drop_missing)} c·ªôt do thi·∫øu d·ªØ li·ªáu > 80%.\")\n",
    "\n",
    "\n",
    "# === 2. L·ªçc c√°c c·ªôt c√≥ gi√° tr·ªã duy nh·∫•t (√≠t bi·∫øn thi√™n) ===\n",
    "\n",
    "# C·∫≠p nh·∫≠t l·∫°i train_df_for_selection sau khi ƒë√£ drop ·ªü b∆∞·ªõc 1\n",
    "train_df_for_selection = df_final[df_final['TARGET'].notnull()].copy()\n",
    "\n",
    "unique_counts = train_df_for_selection.nunique()\n",
    "cols_to_drop_unique = unique_counts[unique_counts == 1].index\n",
    "\n",
    "# √Åp d·ª•ng l·ªçc tr√™n df_final\n",
    "df_final = df_final.drop(columns=cols_to_drop_unique)\n",
    "print(f\"ƒê√£ lo·∫°i b·ªè {len(cols_to_drop_unique)} c·ªôt do ch·ªâ c√≥ 1 gi√° tr·ªã duy nh·∫•t.\")\n",
    "\n",
    "\n",
    "print(f\"--- K√≠ch th∆∞·ªõc df_final sau L·ªçc 'Th√¥': {df_final.shape} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e433f542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ import RandomForestClassifier v√† SelectFromModel t·ª´ sklearn.\n",
      "K√≠ch th∆∞·ªõc t·∫≠p train ƒë·ªÉ ch·ªçn l·ªçc: (307511, 545)\n",
      "S·ªë l∆∞·ª£ng features ban ƒë·∫ßu: 543\n",
      "ƒê√£ x·ª≠ l√Ω NaN v√† Infinity cho X_train.\n"
     ]
    }
   ],
   "source": [
    "print(\"ƒê√£ import RandomForestClassifier v√† SelectFromModel t·ª´ sklearn.\")\n",
    "\n",
    "# 1. T√°ch l·∫°i t·∫≠p train (ch·ªâ train m·ªõi c√≥ TARGET)\n",
    "train_df = df_final[df_final['TARGET'].notnull()].copy()\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p train ƒë·ªÉ ch·ªçn l·ªçc: {train_df.shape}\")\n",
    "\n",
    "# 2. X·ª≠ l√Ω t√™n c·ªôt (cho an to√†n)\n",
    "train_df.columns = [re.sub(r'[^A-Za-z0-9_]+', '', col) for col in train_df.columns]\n",
    "\n",
    "# 3. ƒê·ªãnh nghƒ©a X (features) v√† y (target)\n",
    "y_train = train_df['TARGET']\n",
    "cols_to_drop = ['TARGET', 'SK_ID_CURR']\n",
    "X_train = train_df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"S·ªë l∆∞·ª£ng features ban ƒë·∫ßu: {X_train.shape[1]}\")\n",
    "\n",
    "# 4. X·ª¨ L√ù NaN/Infinity (B·∫ÆT BU·ªòC cho RandomForest)\n",
    "X_train.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "X_train.fillna(0, inplace=True)\n",
    "\n",
    "print(\"ƒê√£ x·ª≠ l√Ω NaN v√† Infinity cho X_train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0341411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.1: Train RandomForest Selector...\n",
      "ƒêang training m√¥ h√¨nh RandomForest... (C√≥ th·ªÉ m·∫•t v√†i ph√∫t)\n",
      "Training ho√†n t·∫•t.\n"
     ]
    }
   ],
   "source": [
    "print(\"9.1: Train RandomForest Selector...\")\n",
    "\n",
    "# 1. Kh·ªüi t·∫°o RandomForest\n",
    "rf_selector = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"ƒêang training m√¥ h√¨nh RandomForest... (C√≥ th·ªÉ m·∫•t v√†i ph√∫t)\")\n",
    "rf_selector.fit(X_train, y_train)\n",
    "print(\"Training ho√†n t·∫•t.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4828d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.2: Ch·ªçn l·ªçc t·ª± ƒë·ªông (Threshold='median')...\n",
      "ƒê√£ t·∫°o b·ªô ch·ªçn (selector) v·ªõi ng∆∞·ª°ng = 'median'.\n",
      "\n",
      "--- Ph√¢n t√≠ch ch·ªçn l·ªçc ---\n",
      "T·ªïng s·ªë features ban ƒë·∫ßu: 543\n",
      "S·ªë l∆∞·ª£ng features quan tr·ªçng (tr√™n m·ª©c trung v·ªã) ƒë∆∞·ª£c gi·ªØ l·∫°i: 272\n",
      "\n",
      "--- Features ƒë∆∞·ª£c ch·ªçn: ---\n",
      "                                              feature  importance\n",
      "109                                  EXT_SOURCES_MEAN    0.022812\n",
      "111                                   EXT_SOURCES_MIN    0.017340\n",
      "112                                   EXT_SOURCES_MAX    0.016145\n",
      "28                                       EXT_SOURCE_2    0.014417\n",
      "6                                          DAYS_BIRTH    0.008653\n",
      "..                                                ...         ...\n",
      "142     NAME_EDUCATION_TYPE_Secondarysecondaryspecial    0.000910\n",
      "530  POS_NAME_CONTRACT_STATUS_RETURNEDTOTHESTORE_MEAN    0.000908\n",
      "179                WEEKDAY_APPR_PROCESS_START_TUESDAY    0.000906\n",
      "121                                 FLAG_OWN_REALTY_Y    0.000899\n",
      "274                 BUREAU_AMT_CREDIT_SUM_OVERDUE_SUM    0.000884\n",
      "\n",
      "[272 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"9.2: Ch·ªçn l·ªçc t·ª± ƒë·ªông (Threshold='median')...\")\n",
    "\n",
    "# 1. Kh·ªüi t·∫°o SelectFromModel\n",
    "# ƒê∆∞a m√¥ h√¨nh ƒë√£ train (rf_selector) v√†o\n",
    "# 'threshold=\"median\"': T·ª± ƒë·ªông t√≠nh to√°n ƒë·ªô quan tr·ªçng trung v·ªã \n",
    "#                       v√† ch·ªâ gi·ªØ l·∫°i c√°c feature c√≥ ƒë·ªô quan tr·ªçng cao h∆°n.\n",
    "# 'prefit=True': B√°o cho SelectFromModel bi·∫øt ch√∫ng ta ƒë√£ train rf_selector r·ªìi.\n",
    "selector = SelectFromModel(rf_selector, threshold=\"median\", prefit=True)\n",
    "\n",
    "print(f\"ƒê√£ t·∫°o b·ªô ch·ªçn (selector) v·ªõi ng∆∞·ª°ng = 'median'.\")\n",
    "\n",
    "# 2. L·∫•y danh s√°ch c√°c feature quan tr·ªçng\n",
    "# .get_support() tr·∫£ v·ªÅ m·ªôt m·∫£ng True/False cho m·ªói feature\n",
    "feature_mask = selector.get_support()\n",
    "# L·∫•y t√™n c·ªßa c√°c feature ƒë∆∞·ª£c ch·ªçn (l√† True)\n",
    "important_features = X_train.columns[feature_mask]\n",
    "\n",
    "# Chuy·ªÉn v·ªÅ d·∫°ng list\n",
    "important_features = important_features.tolist()\n",
    "\n",
    "print(f\"\\n--- Ph√¢n t√≠ch ch·ªçn l·ªçc ---\")\n",
    "print(f\"T·ªïng s·ªë features ban ƒë·∫ßu: {X_train.shape[1]}\")\n",
    "print(f\"S·ªë l∆∞·ª£ng features quan tr·ªçng (tr√™n m·ª©c trung v·ªã) ƒë∆∞·ª£c gi·ªØ l·∫°i: {len(important_features)}\")\n",
    "\n",
    "print(\"\\n--- Features ƒë∆∞·ª£c ch·ªçn: ---\")\n",
    "# Ch√∫ng ta c√≥ th·ªÉ xem l·∫°i DataFrame c≈© ƒë·ªÉ th·∫•y\n",
    "features_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_selector.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "print(features_df[features_df['feature'].isin(important_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce67b33",
   "metadata": {},
   "source": [
    "## 10. X·ª≠ l√Ω Missing value, C√¢n b·∫±ng d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "230b844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Th∆∞ vi·ªán c·∫ßn thi·∫øt cho c√°c b∆∞·ªõc ti·∫øp theo\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57a0d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- B∆Ø·ªöC 10: X·ª≠ l√Ω NaN, Chia Train/Test & C√¢n b·∫±ng D·ªØ li·ªáu ---\n",
      "K√≠ch th∆∞·ªõc t·∫≠p Train ƒë√£ l·ªçc (v·∫´n c√≤n NaN): (307511, 545)\n",
      "\n",
      "K√≠ch th∆∞·ªõc t·∫≠p train nh·ªè: (49202, 543), Test nh·ªè: (6150, 543)\n",
      "‚úÖ Train nh·ªè ƒë√£ ƒë∆∞·ª£c c√¢n b·∫±ng b·∫±ng UnderSampling. K√≠ch th∆∞·ªõc m·ªõi:\n",
      "TARGET\n",
      "0.0    3958\n",
      "1.0    3958\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- HO√ÄN T·∫§T QUY TR√åNH FEATURE ENGINEERING & TI·ªÄN M√î H√åNH ---\n",
      "üíæ Train balanced ƒë√£ l∆∞u: (7916, 544)\n",
      "üíæ Test modeling ƒë√£ l∆∞u: (6150, 544)\n"
     ]
    }
   ],
   "source": [
    "print(\"--- B∆Ø·ªöC 10: X·ª≠ l√Ω NaN, Chia Train/Test & C√¢n b·∫±ng D·ªØ li·ªáu ---\")\n",
    "\n",
    "# S·ª≠a ƒë·ªïi logic ƒë·ªÉ gi·ªØ l·∫°i SK_ID_CURR trong t·∫≠p features khi ƒëi·ªÅn NaN\n",
    "\n",
    "# 1. ƒê·ªãnh nghƒ©a l·∫°i t·∫≠p train v√† test\n",
    "train_df_filtered = df_final[df_final['TARGET'].notnull()].copy()\n",
    "test_df_filtered = df_final[df_final['TARGET'].isnull()].copy()\n",
    "print(f\"K√≠ch th∆∞·ªõc t·∫≠p Train ƒë√£ l·ªçc (v·∫´n c√≤n NaN): {train_df_filtered.shape}\")\n",
    "\n",
    "\n",
    "# 2. X·ª¨ L√ù MISSING VALUE TR√äN T·∫¨P TRAIN (Imputation) \n",
    "X_all_features = train_df_filtered.drop(columns=['TARGET']).copy()\n",
    "y_train = train_df_filtered['TARGET']\n",
    "\n",
    "X_features_only = X_all_features.drop(columns=['SK_ID_CURR'])\n",
    "\n",
    "X_filled_features = X_features_only.fillna(X_features_only.mean())\n",
    "\n",
    "for col in X_filled_features.select_dtypes(include=['object']).columns:\n",
    "    X_filled_features[col] = X_filled_features[col].fillna(X_filled_features[col].mode()[0]) \n",
    "\n",
    "# Th√™m l·∫°i SK_ID_CURR v√†o t·∫≠p features ƒë√£ fill\n",
    "X_filled_features['SK_ID_CURR'] = X_all_features['SK_ID_CURR'].values \n",
    "\n",
    "# clean_df gi·ªù ƒë√¢y s·∫Ω bao g·ªìm t·∫•t c·∫£ features, SK_ID_CURR, v√† TARGET\n",
    "clean_df = pd.concat(\n",
    "    [X_filled_features.reset_index(drop=True), y_train.reset_index(drop=True)], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# 3. CHIA L·∫†I TRAIN/TEST \n",
    "X = clean_df.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = clean_df[\"TARGET\"]\n",
    "\n",
    "# Chia d·ªØ li·ªáu train/test (80% - 20%)\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Gi·∫£m k√≠ch th∆∞·ªõc (Sampling, theo logic c·ªßa AI_PROJECT.ipynb)\n",
    "train_frac = 0.2 \n",
    "test_frac = 0.1\n",
    "\n",
    "X_train_small = X_train_full.sample(frac=train_frac, random_state=42)\n",
    "y_train_small = y_train_full.loc[X_train_small.index]\n",
    "\n",
    "X_test_small = X_test_full.sample(frac=test_frac, random_state=42)\n",
    "y_test_small = y_test_full.loc[X_test_small.index]\n",
    "\n",
    "print(f\"\\nK√≠ch th∆∞·ªõc t·∫≠p train nh·ªè: {X_train_small.shape}, Test nh·ªè: {X_test_small.shape}\")\n",
    "\n",
    "# 4. C√ÇN B·∫∞NG D·ªÆ LI·ªÜU (UnderSampling)\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train_small, y_train_small)\n",
    "\n",
    "print(\"‚úÖ Train nh·ªè ƒë√£ ƒë∆∞·ª£c c√¢n b·∫±ng b·∫±ng UnderSampling. K√≠ch th∆∞·ªõc m·ªõi:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# 5. X·ª¨ L√ù MISSING VALUE TR√äN T·∫¨P TEST (√Åp d·ª•ng Mean t·ª´ t·∫≠p Train g·ªëc)\n",
    "# T√°ch X_test_final (t·∫≠p test nh·ªè) v√† x·ª≠ l√Ω NaN b·∫±ng Mean t·ª´ T·∫¨P TRAIN g·ªëc (X_train)\n",
    "X_test_final = X_test_small.fillna(X_train.mean()) # D√πng mean c·ªßa t·∫≠p train g·ªëc\n",
    "\n",
    "# G·ªôp l·∫°i th√†nh DataFrame Test cu·ªëi c√πng (ƒë√£ clean)\n",
    "test_df_final = pd.concat(\n",
    "    [X_test_final.reset_index(drop=True),\n",
    "     y_test_small.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# 6. XU·∫§T FILE CU·ªêI C√ôNG\n",
    "train_balanced_df = pd.concat(\n",
    "    [pd.DataFrame(X_train_resampled, columns=X.columns),\n",
    "     pd.Series(y_train_resampled, name='TARGET')],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# S·ª≠ d·ª•ng m·ªôt ƒë∆∞·ªùng d·∫´n chung (PATH ƒë√£ ƒë·ªãnh nghƒ©a ·ªü ƒë·∫ßu file)\n",
    "output_path_train_balanced = PATH + \"/train_balanced_under.csv\"\n",
    "output_path_test_small = PATH + \"/test_modeling_small.csv\" # ƒê·ªïi t√™n cho r√µ r√†ng h∆°n\n",
    "\n",
    "train_balanced_df.to_csv(output_path_train_balanced, index=False)\n",
    "test_df_final.to_csv(output_path_test_small, index=False)\n",
    "\n",
    "\n",
    "print(\"\\n--- HO√ÄN T·∫§T QUY TR√åNH FEATURE ENGINEERING & TI·ªÄN M√î H√åNH ---\")\n",
    "print(f\"üíæ Train balanced ƒë√£ l∆∞u: {train_balanced_df.shape}\")\n",
    "print(f\"üíæ Test modeling ƒë√£ l∆∞u: {test_df_final.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
